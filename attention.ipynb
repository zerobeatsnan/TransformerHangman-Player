{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.nn import L1Loss\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import itertools\n",
    "import random\n",
    "from tqdm import trange\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from scipy.special import comb\n",
    "from transform_encoder import InputEmbedding, PositionEmbedding, LayerNormalization\n",
    "from transform_encoder import  FeedForwardSec, MultiHeadAttentionSec, SkipConnection,EncoderSec, Encoder\n",
    "# import ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "cuda_available = torch.cuda.is_available()\n",
    "\n",
    "# If CUDA is available, set the default device to GPU\n",
    "if cuda_available:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "full_dictionary_location = \"words_250000_train.txt\"\n",
    "\n",
    "def build_dictionary(dictionary_file_location):\n",
    "    text_file = open(dictionary_file_location,\"r\")\n",
    "    full_dictionary = text_file.read().splitlines()\n",
    "    text_file.close()\n",
    "    return list(full_dictionary)\n",
    "\n",
    "words_list = build_dictionary(full_dictionary_location)[:]\n",
    "chars = sorted(list(set(''.join(words_list) + \"_\")))\n",
    "vocab_size = len(chars)\n",
    "chars = chars[1:]\n",
    "chars.append(\"_\")\n",
    "alphabet_chars = sorted(list(set(''.join(words_list))))\n",
    "alphabet_size = len(alphabet_chars)\n",
    "char_to_num_input = {char: i for i, char in enumerate(chars)}\n",
    "char_to_num_output = {char: i for i, char in enumerate(alphabet_chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from math import comb\n",
    "\n",
    "class WordsDataset(Dataset):\n",
    "    def __init__(self, words, char_to_index, max_missing=3):\n",
    "        self.words = words\n",
    "        self.char_to_index = char_to_index\n",
    "        self.vocab_size = len(char_to_index) - 1  # Exclude \"_\" from target vocab size\n",
    "        self.max_missing = max_missing\n",
    "        self.samples = self.prepare_dataset()\n",
    "\n",
    "    def prepare_dataset(self):\n",
    "        samples = []\n",
    "        max_combinations_per_word = 1000\n",
    "        for word in tqdm(self.words, desc=\"Preparing dataset\"):\n",
    "            unique_chars = list(set(word))  # Get unique characters to handle repeats consistently\n",
    "            word_len = len(word)\n",
    "\n",
    "            if word_len == 1:\n",
    "                encoded_sample, target = self.create_sample_target(word, [0])\n",
    "                samples.append((encoded_sample, target, [0]))\n",
    "            else:\n",
    "                generated_combinations = set()\n",
    "                # Attempt to generate up to max_combinations_per_word unique combinations\n",
    "                for _ in range(max_combinations_per_word):\n",
    "                    num_chars_to_mask = random.randint(1, min(len(unique_chars), max_combinations_per_word))\n",
    "                    chars_to_mask = random.sample(unique_chars, num_chars_to_mask)\n",
    "\n",
    "                    # Create a unique identifier for this combination of characters\n",
    "                    combination_id = ''.join(sorted(chars_to_mask))\n",
    "                    if combination_id not in generated_combinations:\n",
    "                        indices_to_mask = [i for i, char in enumerate(word) if char in chars_to_mask]\n",
    "                        if len(indices_to_mask) > word_len * 0.7:\n",
    "                            continue  # Skip this combination\n",
    "\n",
    "                        encoded_sample, target = self.create_sample_target(word, indices_to_mask)\n",
    "                        samples.append((encoded_sample, target, indices_to_mask))\n",
    "                        generated_combinations.add(combination_id)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def create_sample_target(self, word, missing_indices):\n",
    "        sample = [self.char_to_index[char] if i not in missing_indices else self.char_to_index[\"_\"] for i, char in enumerate(word)]\n",
    "        target = [self.char_to_index[word[i]] for i in range(len(word))]\n",
    "        return np.array(sample), np.array(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample, target, missing_indices = self.samples[idx]\n",
    "        sample_tensor = torch.tensor(sample, dtype=torch.long)\n",
    "        target_tensor = torch.tensor(target, dtype=torch.long)\n",
    "        exist_mask = torch.zeros(len(sample), dtype=torch.bool)\n",
    "        for i in missing_indices:\n",
    "            exist_mask[i] = 1\n",
    "        return sample_tensor, target_tensor, exist_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preparing dataset: 100%|██████████| 227300/227300 [34:17<00:00, 110.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "full_dataset = WordsDataset(words_list, char_to_num_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the splits\n",
    "train_size = int(0.95 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "# Randomly split the dataset into training and validation datasets\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_dataset(dataset, file_path):\n",
    "    with open(file_path, 'wb') as f:\n",
    "        pickle.dump(dataset, f)\n",
    "\n",
    "# Assuming `dataset` is an instance of WordsDataset\n",
    "save_dataset(full_dataset, 'words_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    return dataset\n",
    "\n",
    "# Load the dataset\n",
    "full_dataset = load_dataset('words_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(Predictor, self).__init__()\n",
    "        # Linear layer to project from d_model to vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, exist_mask=None):\n",
    "        # Apply the linear layer to project from d_model to vocab_size\n",
    "        logits = self.linear(x)\n",
    "        \n",
    "        # If exist_mask is provided and needs to be used in a specific way, handle accordingly\n",
    "        # For example, if you need to focus on masked positions for a specific computation\n",
    "        # Adjust the handling of exist_mask here as per the requirements\n",
    "        \n",
    "        # Without modifying logits based on exist_mask, simply return log softmax of logits\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "    \n",
    "    \n",
    "class ImprovedPredictor(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(ImprovedPredictor, self).__init__()\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x, exist_mask=None):\n",
    "        # Apply the linear layer to project from d_model to vocab_size\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        if exist_mask is not None:\n",
    "            # Ensure exist_mask is broadcastable to logits shape\n",
    "            exist_mask = exist_mask.unsqueeze(-1).expand_as(logits)\n",
    "\n",
    "            # Apply a large negative value to positions not to be predicted (where exist_mask == 0)\n",
    "            # This effectively removes them from consideration in the max pooling step\n",
    "            modified_logits = logits.masked_fill_(exist_mask == 0, -1e9)\n",
    "            \n",
    "            # Apply max pooling across the sequence length dimension (dim=1)\n",
    "            # Keepdim=True maintains the original number of dimensions\n",
    "            pooled_logits, _ = torch.max(modified_logits, dim=1, keepdim=True)\n",
    "            \n",
    "            # Normalize pooled logits across the vocabulary dimension to get probabilities\n",
    "            probs = F.softmax(pooled_logits, dim=-1)\n",
    "        else:\n",
    "            # If no mask is provided, compute softmax probabilities directly from logits\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "\n",
    "        return probs\n",
    "    \n",
    "    \n",
    "class RefinedPredictor(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(RefinedPredictor, self).__init__()\n",
    "        # Linear layer to project from d_model to vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear layer to get logits\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # Apply max pooling across the sequence length dimension (dim=1)\n",
    "        # This step leverages information from the entire sequence\n",
    "        pooled_logits, _ = torch.max(logits, dim=1, keepdim=True)\n",
    "\n",
    "        # Reshape pooled logits to ensure the output shape is [batch_size, vocab_size]\n",
    "        # This is crucial for compatibility with the expected output format\n",
    "        reshaped_pooled_logits = pooled_logits.squeeze(1)\n",
    "\n",
    "        # Normalize pooled logits across the vocabulary dimension to get probabilities\n",
    "        probs = F.softmax(reshaped_pooled_logits, dim=-1)\n",
    "\n",
    "        return probs\n",
    "    \n",
    "class RefinedPredictor_v2(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(RefinedPredictor_v2, self).__init__()\n",
    "        # Linear layer to project from d_model to vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear layer to get logits\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # Apply average pooling across the sequence length dimension (dim=1)\n",
    "        # This step aggregates information from the entire sequence by averaging\n",
    "        pooled_logits = torch.mean(logits, dim=1, keepdim=True)\n",
    "\n",
    "        # Reshape pooled logits to ensure the output shape is [batch_size, vocab_size]\n",
    "        reshaped_pooled_logits = pooled_logits.squeeze(1)\n",
    "\n",
    "        # Normalize pooled logits across the vocabulary dimension to get probabilities\n",
    "        probs = F.softmax(reshaped_pooled_logits, dim=-1)\n",
    "\n",
    "        return probs\n",
    "    \n",
    "\n",
    "\n",
    "class RefinedPredictor_v2_logit(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super(RefinedPredictor_v2, self).__init__()\n",
    "        # Linear layer to project from d_model to vocab_size\n",
    "        self.linear = nn.Linear(in_features=d_model, out_features=vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the linear layer to get logits\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        # Apply average pooling across the sequence length dimension (dim=1)\n",
    "        # This step aggregates information from the entire sequence by averaging\n",
    "        pooled_logits = torch.mean(logits, dim=1, keepdim=True)\n",
    "\n",
    "        # Reshape pooled logits to ensure the output shape is [batch_size, vocab_size]\n",
    "        reshaped_pooled_logits = pooled_logits.squeeze(1)\n",
    "\n",
    "        # Return the reshaped pooled logits directly\n",
    "        return reshaped_pooled_logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_focused_loss(logits, targets, exist_mask):\n",
    "    # This is a placeholder for the concept; actual implementation may vary\n",
    "    # Only select logits and targets where exist_mask == 1\n",
    "    \n",
    "    # print(\"logits have shape:\", logits.shape)\n",
    "    # print(\"targets have shape:\", targets.shape)\n",
    "    relevant_logits = logits[exist_mask.unsqueeze(-1).expand_as(logits)].view(-1, logits.size(-1))\n",
    "    relevant_targets = targets[exist_mask]\n",
    "    \n",
    "    # print(\"relevant_logits have shape:\",relevant_logits.shape)\n",
    "    # print(\"relevant_targets have shape:\", relevant_targets.shape)\n",
    "    # ipdb.set_trace()\n",
    "    # Calculate loss on these positions only\n",
    "    loss = F.cross_entropy(relevant_logits, relevant_targets)\n",
    "    return loss\n",
    "\n",
    "def calculate_improved_loss(logits, targets, exist_mask):\n",
    "    # logits: Model predictions, shape [batch_size, seq_len, vocab_size]\n",
    "    # targets: Target character indices, shape [batch_size, seq_len]\n",
    "    # exist_mask: Mask indicating missing character positions, shape [batch_size, seq_len]\n",
    "\n",
    "    batch_size, seq_len, vocab_size = logits.size()\n",
    "    \n",
    "    # Flatten to work with indices directly\n",
    "    logits_flattened = logits.view(-1, vocab_size)\n",
    "    targets_flattened = targets.view(-1)\n",
    "    exist_mask_flattened = exist_mask.view(-1)\n",
    "    \n",
    "    # Filter logits and targets for missing positions only\n",
    "    missing_logits = logits_flattened[exist_mask_flattened]\n",
    "    missing_targets = targets_flattened[exist_mask_flattened]\n",
    "    \n",
    "    # Convert targets to one-hot encoded form to represent true distribution\n",
    "    true_dist = torch.zeros_like(missing_logits).scatter_(1, missing_targets.unsqueeze(1), 1)\n",
    "    \n",
    "    # Max pooling across the missing positions for each character\n",
    "    max_logits, _ = torch.max(missing_logits, dim=0, keepdim=True)\n",
    "    max_logits_expanded = max_logits.expand_as(missing_logits)\n",
    "    \n",
    "    # Normalize the max pooled logits to get a probability distribution\n",
    "    pred_prob = F.softmax(max_logits_expanded, dim=-1)\n",
    "    \n",
    "    # Calculate KL Divergence between normalized predictions and true distribution\n",
    "    loss = F.kl_div(F.log_softmax(pred_prob, dim=-1), true_dist, reduction='batchmean')\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def kl_div_loss(output_probs, targets, exist_mask):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence loss between model output probabilities and target distribution\n",
    "    based on the frequency of missing characters.\n",
    "\n",
    "    Parameters:\n",
    "    - output_probs: Tensor [batch_size, vocab_size] - probabilities for each character from the model.\n",
    "    - targets: Tensor [batch_size, seq_len] - indices of all characters in each sequence.\n",
    "    - exist_mask: Tensor [batch_size, seq_len] - indicates positions of missing characters (1 for missing).\n",
    "\n",
    "    Returns:\n",
    "    - Loss value as a scalar tensor.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = targets.size()\n",
    "    vocab_size = output_probs.size(1)\n",
    "    device = output_probs.device\n",
    "    \n",
    "    # Initialize a tensor to store the target distributions\n",
    "    target_distributions = torch.zeros(batch_size, vocab_size, device=device)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        # Identify the indices of missing characters for this sequence\n",
    "        missing_indices = exist_mask[i].bool()\n",
    "        missing_chars = targets[i, missing_indices]\n",
    "        \n",
    "        # Build the frequency distribution of missing characters\n",
    "        for char_idx in missing_chars:\n",
    "            target_distributions[i, char_idx] += 1\n",
    "        \n",
    "        # Normalize the distribution to sum to 1\n",
    "        target_distributions[i] /= target_distributions[i].sum()\n",
    "    \n",
    "    # Calculate the KL divergence loss\n",
    "    loss = F.kl_div(output_probs.log(), target_distributions, reduction='batchmean')\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_presence_distribution(targets, mask):\n",
    "    \"\"\"\n",
    "    Creates a presence/absence distribution tensor based on characters selected by the mask.\n",
    "\n",
    "    Parameters:\n",
    "    - targets: Tensor of shape [batch_size, seq_len] with character indices.\n",
    "    - mask: Tensor of shape [batch_size, seq_len] indicating characters to consider (1) or ignore (0).\n",
    "\n",
    "    Returns:\n",
    "    - A tensor of shape [batch_size, 26] where each element is 1 if the character is present and 0 otherwise.\n",
    "    \"\"\"\n",
    "    device = targets.device\n",
    "    batch_size, seq_len = targets.size()\n",
    "    vocab_size = 26  # Fixed size for the alphabet\n",
    "\n",
    "    # Initialize the distribution tensor\n",
    "    target_distribution = torch.zeros(batch_size, vocab_size, dtype=torch.float, device=device)\n",
    "    \n",
    "    # Apply the mask to filter targets\n",
    "    filtered_targets = targets * mask\n",
    "    \n",
    "    # Flatten the filtered targets and create a corresponding batch index tensor\n",
    "    flat_filtered_targets = filtered_targets.flatten()\n",
    "    flat_batch_indices = torch.arange(batch_size, device=device).unsqueeze(1).expand(-1, seq_len).flatten()\n",
    "    \n",
    "    # Filter out zeros added by masking\n",
    "    valid_indices = flat_filtered_targets.nonzero().squeeze()\n",
    "    valid_targets = flat_filtered_targets[valid_indices]\n",
    "    valid_batch_indices = flat_batch_indices[valid_indices]\n",
    "    \n",
    "    # Accumulate counts in the target distribution tensor\n",
    "    target_distribution.index_put_((valid_batch_indices, valid_targets), \n",
    "                                   torch.ones_like(valid_targets, dtype=torch.float), \n",
    "                                   accumulate=True)\n",
    "    \n",
    "    # Convert counts to presence (1) or absence (0)\n",
    "    presence_distribution = (target_distribution > 0).float()\n",
    "\n",
    "    return presence_distribution\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_costum(output_probs, targets, exist_mask):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence loss between model output probabilities and target distribution\n",
    "    based on the frequency of missing characters, avoiding explicit loops.\n",
    "\n",
    "    Parameters:\n",
    "    - output_probs: Tensor [batch_size, vocab_size] - probabilities for each character from the model.\n",
    "    - targets: Tensor [batch_size, seq_len] - indices of all characters in each sequence.\n",
    "    - exist_mask: Tensor [batch_size, seq_len] - indicates positions of missing characters (1 for missing).\n",
    "\n",
    "    Returns:\n",
    "    - Loss value as a scalar tensor.\n",
    "    \"\"\"\n",
    "    \n",
    "    frequency = create_presence_distribution(targets, exist_mask)\n",
    "    \n",
    "    loss = F.binary_cross_entropy(output_probs, frequency)\n",
    "\n",
    "    \n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def kl_div_loss_fast(output_probs, targets, exist_mask):\n",
    "    \"\"\"\n",
    "    Calculate the KL divergence loss between model output probabilities and target distribution\n",
    "    based on the frequency of missing characters, avoiding explicit loops.\n",
    "\n",
    "    Parameters:\n",
    "    - output_probs: Tensor [batch_size, vocab_size] - probabilities for each character from the model.\n",
    "    - targets: Tensor [batch_size, seq_len] - indices of all characters in each sequence.\n",
    "    - exist_mask: Tensor [batch_size, seq_len] - indicates positions of missing characters (1 for missing).\n",
    "\n",
    "    Returns:\n",
    "    - Loss value as a scalar tensor.\n",
    "    \"\"\"\n",
    "    batch_size, seq_len = targets.size()\n",
    "    vocab_size = output_probs.size(1)\n",
    "    device = output_probs.device\n",
    "\n",
    "    # Create a flat index for batch elements\n",
    "    batch_indices = torch.arange(batch_size).unsqueeze(1).expand(-1, seq_len).reshape(-1)\n",
    "    # Flatten targets and mask to use for filtering missing characters\n",
    "    flat_targets = targets.reshape(-1)\n",
    "    flat_mask = exist_mask.reshape(-1)\n",
    "\n",
    "    # Filter to get indices and targets of missing characters only\n",
    "    missing_indices = batch_indices[flat_mask]\n",
    "    missing_targets = flat_targets[flat_mask]\n",
    "\n",
    "    # Initialize tensor to store the target distributions\n",
    "    target_distributions = torch.zeros(batch_size, vocab_size, device=device)\n",
    "\n",
    "    # Use scatter_add_ to accumulate the counts of each character directly into the target distribution\n",
    "    target_distributions.index_put_((missing_indices, missing_targets), torch.ones_like(missing_targets, dtype=torch.float), accumulate=True)\n",
    "\n",
    "    # Normalize each row to sum to 1 to get proper distributions\n",
    "    target_distributions /= target_distributions.sum(dim=1, keepdim=True)\n",
    "\n",
    "    # Calculate the KL divergence loss\n",
    "    loss = F.kl_div(output_probs.log(), target_distributions, reduction='batchmean')\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class HangmanPredictor(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, output_size, n_enc_layers=4):\n",
    "        super(HangmanPredictor, self).__init__()\n",
    "        # Initialize embeddings and encoder as before\n",
    "        self.input_embedding = InputEmbedding(d_model=d_model, vocab_size=vocab_size)\n",
    "        self.position_embedding = PositionEmbedding(d_model=d_model, dropout=0.1, max_len=1000)\n",
    "\n",
    "        self_attention_layer = MultiHeadAttentionSec(d_model, h = 4, dropout=0.1)\n",
    "        feed_forward_layer = FeedForwardSec(d_model, d_ff = 512, dropout=0.1)\n",
    "        encoder_layer = EncoderSec(self_attention_layer, feed_forward_layer, dropout=0.1)\n",
    "        self.encoder = Encoder(nn.ModuleList([encoder_layer for _ in range(n_enc_layers)]))\n",
    "\n",
    "        self.predictor = RefinedPredictor(d_model = d_model, vocab_size = output_size)\n",
    "\n",
    "    def forward(self, x, mask=None, exist_mask=None):\n",
    "        # Apply input and position embeddings\n",
    "        x = self.input_embedding(x)\n",
    "        x = self.position_embedding(x)\n",
    "        \n",
    "        # Pass through the encoder layers\n",
    "        x = self.encoder(x, mask)\n",
    "        \n",
    "        # Generate predictions using the predictor\n",
    "        predictions = self.predictor(x)\n",
    "        return predictions\n",
    "    \n",
    "    @property\n",
    "    def device(self):\n",
    "        # Return the device of the model's parameters\n",
    "        return next(self.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(model, data_loader, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(data_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=True)\n",
    "        for sample, target, exist_mask in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Adjust dimensions if necessary\n",
    "            sample = sample.to(model.device)\n",
    "            target = target.to(model.device)\n",
    "            exist_mask = exist_mask.to(model.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(sample)\n",
    "            # print(\"The shape of output is:\", output.shape)\n",
    "            \n",
    "            # Compute custom loss\n",
    "            # loss = calculate_focused_loss(output, target, exist_mask)\n",
    "            loss = calculate_improved_loss(output, target, exist_mask)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_v2(model, train_loader, val_loader, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0.0\n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs}', leave=True)\n",
    "        \n",
    "        for sample, target, exist_mask in progress_bar:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Adjust dimensions if necessary\n",
    "            sample = sample.to(model.device)\n",
    "            target = target.to(model.device)\n",
    "            exist_mask = exist_mask.to(model.device)\n",
    "            \n",
    "            # Forward pass through the model to get output probabilities\n",
    "            output_probs = model(sample)\n",
    "            \n",
    "            # Ensure the output is compatible with the KL divergence loss expectations\n",
    "            # This might require you to adjust either the model's output or how you handle it here,\n",
    "            # depending on the output format of your ImprovedPredictor.\n",
    "            \n",
    "            # Compute the KL divergence loss\n",
    "            loss = kl_div_loss_fast(output_probs, target, exist_mask)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        # Validation Phase\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        with torch.no_grad():  # No need to track gradients during validation\n",
    "            val_progress_bar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]', leave=True)\n",
    "            for sample, target, exist_mask in val_progress_bar:\n",
    "                sample = sample.to(model.device)\n",
    "                target = target.to(model.device)\n",
    "                exist_mask = exist_mask.to(model.device)\n",
    "                \n",
    "                output_probs = model(sample)\n",
    "                val_loss = kl_div_loss_fast(output_probs, target, exist_mask)\n",
    "                \n",
    "                total_val_loss += val_loss.item()\n",
    "                \n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Training Loss: {avg_train_loss}, Validation Loss: {avg_val_loss}\")\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    samples, targets, exist_masks = zip(*batch)\n",
    "\n",
    "    # Pad samples and exist_masks to have the same length\n",
    "    # samples_padded = pad_sequence([torch.tensor(s, dtype=torch.long) for s in samples], batch_first=True, padding_value=0)\n",
    "    # targets_padded = pad_sequence([torch.tensor(t, dtype=torch.long) for t in targets], batch_first=True, padding_value=-1)  # Using -1 as padding value for targets\n",
    "    samples_padded = pad_sequence([s.clone().detach() for s in samples], batch_first=True, padding_value=0)\n",
    "    targets_padded = pad_sequence([t.clone().detach() for t in targets], batch_first=True, padding_value=-1)\n",
    "\n",
    "\n",
    "    # Since exist_mask indicates positions that should be predicted, we can treat it similarly to targets\n",
    "    #exist_masks_padded = pad_sequence([torch.tensor(em, dtype=torch.bool) for em in exist_masks], batch_first=True, padding_value=False)  # False indicates non-missing positions\n",
    "    exist_masks_padded = pad_sequence([em.clone().detach() for em in exist_masks], batch_first=True, padding_value=False)\n",
    "    return samples_padded, targets_padded, exist_masks_padded\n",
    "\n",
    "batch_size = 3200\n",
    "# Use the custom collate function in your DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn, num_workers = 4, pin_memory =True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "vocab_size = 27  # Including \"_\"\n",
    "d_model = 128\n",
    "output_size = 26\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = HangmanPredictor(vocab_size=vocab_size, d_model=d_model, output_size=output_size,n_enc_layers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.load_state_dict(torch.load(\"attention_model_parameters_v1.pth\")) # d_model = 256 d_ff 1024\n",
    "# model.load_state_dict(torch.load(\"attention_model_parameters_v2.pth\")) # d_model = 256 d_ff = 1024\n",
    "model.load_state_dict(torch.load(\"attention_model_parameters_v3.pth\")) # d_model = 128 d_ff = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/11718 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 5.38 GiB total capacity; 3.97 GiB already allocated; 0 bytes free; 4.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Adam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mtrain_v2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36mtrain_v2\u001b[0;34m(model, train_loader, val_loader, optimizer, epochs)\u001b[0m\n\u001b[1;32m     14\u001b[0m exist_mask \u001b[38;5;241m=\u001b[39m exist_mask\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Forward pass through the model to get output probabilities\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m output_probs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Ensure the output is compatible with the KL divergence loss expectations\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# This might require you to adjust either the model's output or how you handle it here,\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# depending on the output format of your ImprovedPredictor.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Compute the KL divergence loss\u001b[39;00m\n\u001b[1;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m kl_div_loss_fast(output_probs, target, exist_mask)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mHangmanPredictor.forward\u001b[0;34m(self, x, mask, exist_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding(x)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Pass through the encoder layers\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Generate predictions using the predictor\u001b[39;00m\n\u001b[1;32m     24\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/attention hangman/transform_encoder.py:193\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, x, mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,mask):\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 193\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/attention hangman/transform_encoder.py:178\u001b[0m, in \u001b[0;36mEncoderSec.forward\u001b[0;34m(self, x, src_mask)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, src_mask):\n\u001b[1;32m    176\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_connection[\u001b[38;5;241m0\u001b[39m](x, \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attention(x,x,x,src_mask))\n\u001b[0;32m--> 178\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mskip_connection\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_sec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/attention hangman/transform_encoder.py:164\u001b[0m, in \u001b[0;36mSkipConnection.forward\u001b[0;34m(self, x, sublayer)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x,sublayer):\n\u001b[0;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(\u001b[43msublayer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/work/attention hangman/transform_encoder.py:96\u001b[0m, in \u001b[0;36mFeedForwardSec.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# output = self.relu(output)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(output)\n\u001b[0;32m---> 96\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc_2(output)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/dropout.py:58\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/functional.py:1169\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(p))\n\u001b[0;32m-> 1169\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 5.38 GiB total capacity; 3.97 GiB already allocated; 0 bytes free; 4.07 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Move the model to the device\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train\n",
    "train_v2(model=model, train_loader=train_loader, val_loader= val_loader,optimizer=optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"attention_model_parameters_v2.pth\")\n",
    "torch.save(model.state_dict(), \"attention_model_parameters_v3.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.5458e-04, 1.3037e-03, 1.6567e-02, 9.9969e-03, 4.4593e-02, 2.5904e-04,\n",
      "        5.2739e-05, 4.9735e-02, 1.1763e-04, 3.8575e-05, 1.6386e-03, 2.8374e-02,\n",
      "        9.7916e-05, 2.6587e-01, 1.2098e-01, 5.9776e-03, 1.1408e-05, 1.8605e-01,\n",
      "        2.7630e-02, 7.6370e-02, 1.1172e-01, 3.2902e-04, 6.5527e-04, 1.4593e-03,\n",
      "        4.3444e-02, 6.4802e-03])\n",
      "{'a': 0, 'b': 1, 'c': 2, 'd': 3, 'e': 4, 'f': 5, 'g': 6, 'h': 7, 'i': 8, 'j': 9, 'k': 10, 'l': 11, 'm': 12, 'n': 13, 'o': 14, 'p': 15, 'q': 16, 'r': 17, 's': 18, 't': 19, 'u': 20, 'v': 21, 'w': 22, 'x': 23, 'y': 24, 'z': 25, '_': 26}\n",
      "n\n"
     ]
    }
   ],
   "source": [
    "def predict_one_missing_character(hangman_model, guessing_word, excluded_chars, char_to_index, index_to_char):\n",
    "    hangman_model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "    # Convert the guessing word into tensor\n",
    "    input_indices = [char_to_index.get(char, char_to_index['_']) for char in guessing_word]\n",
    "    input_tensor = torch.tensor([input_indices], dtype=torch.long).to(hangman_model.device)\n",
    "\n",
    "    # Predict probabilities\n",
    "    with torch.no_grad():\n",
    "        output_probs = hangman_model(input_tensor)[0]  # Assuming the output is in the desired format\n",
    "        # print(output_probs.shape)\n",
    "        # print(output_probs)\n",
    "    \n",
    "    # Apply softmax to convert logits into probabilities\n",
    "    # probs = F.softmax(output_probs, dim=-1)\n",
    "\n",
    "    # Identify the index of the first missing character\n",
    "    missing_idx = guessing_word.find('_')\n",
    "    if missing_idx == -1:\n",
    "        return None  # No missing character found\n",
    "    \n",
    "    # Mask out excluded characters by setting their probabilities to 0\n",
    "    for char in excluded_chars:\n",
    "        if char in char_to_index:\n",
    "            output_probs[char_to_index[char]] = 0\n",
    "    print(output_probs)\n",
    "    # Find the most probable character for the missing position\n",
    "    max_value_index = torch.argmax(output_probs)\n",
    "    # print(index_to_char)\n",
    "    # print(max_value_index)\n",
    "    print(char_to_index)\n",
    "    predicted_char = index_to_char[max_value_index.item()]\n",
    "\n",
    "    return predicted_char\n",
    "# Example usage, assuming char_to_index and index_to_char are defined properly\n",
    "char_to_index = char_to_num_input = {char: i for i, char in enumerate(chars)}\n",
    "index_to_char = {index: char for char, index in char_to_index.items()}\n",
    "guessing_word = \"m_gi_\"\n",
    "excluded_chars = []\n",
    "predicted_word = predict_one_missing_character(model, guessing_word, excluded_chars, char_to_index, index_to_char)\n",
    "print(predicted_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention parameter v1 is use refined predictor d_model = 256 d_ff = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention parameter v2 is use refined predictor_v2 and cross entrophy d_model = 256, d_ff = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention parameter v3 d_model = 128, d_ff = 512"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
